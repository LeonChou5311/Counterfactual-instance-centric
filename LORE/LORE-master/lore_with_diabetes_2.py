# -*- coding: utf-8 -*-
"""LORE_with_diabetes_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NM2_ZWkxs-Mf5qpFwyJEk247dRXyHQQk
"""

!pip3 install tensorflow
############# install package
!pip install dice_ml
!pip install alibi
!pip install pyAgrum

############# Initialise on Google Colab 

from google.colab import drive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials 
drive.mount('/content/gdrive', force_remount=True)
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
gdrive = GoogleDrive(gauth)
gdrive.CreateFile({"id": "1wwSN3AIl_dmayKENu5jnc1BRaNPe8BZc"}).GetContentFile("learning.py")

# Commented out IPython magic to ensure Python compatibility.
############# import the library
import tensorflow as tf
import dice_ml
import pandas as pd
tf.get_logger().setLevel(40) # suppress deprecation messages
#tf.compat.v1.disable_v2_behavior() # disable TF2 behaviour as alibi code still relies on TF1 constructs
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.utils import to_categorical
import matplotlib
# %matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import os
from sklearn.datasets import load_boston
import pandas as pd
from sklearn.preprocessing import OneHotEncoder, StandardScaler,MinMaxScaler
from learning import encode_data 
from learning import *
print('TF version: ', tf.__version__)
print('Eager execution enabled: ', tf.executing_eagerly()) # False
seed = 123
tf.random.set_seed(seed)
np.random.seed(seed)

from google.colab import drive 
# drive.mount('/content/gdrive') 
data=pd.read_csv('gdrive/My Drive//Counterfactual-prototype-main/datasets/diabetes.csv')
# Giving current root path
# PATH = "./"
PATH = "gdrive/My Drive//Counterfactual-prototype-main/"
# PATH = "/Counterfactual-prototype-main/"
# name of dataset
DATASET_NAME = "diabetes.csv"

# variable containing the class labels in this case the dataset contains:
# 0 - if not diabetes
# 1 - if diabetes
class_var = "Outcome"

# load dataset
dataset_path = PATH + "datasets/" + DATASET_NAME
data = pd.read_csv( dataset_path )

# features
feature_names = data.drop([class_var], axis=1).columns.to_list()

# balance dataset
sampled_data = data.sample(frac=1)
sampled_data = sampled_data[ sampled_data["Outcome"] == 0]

no_data = sampled_data.sample(frac=1)[0:268]
yes_data = data[ data["Outcome"] == 1]

balanced_data = [no_data,yes_data]
balanced_data = pd.concat(balanced_data)

# apply one hot encoder to data
# standardize the input between 0 and 1
X, Y, encoder, scaler = encode_data( balanced_data, class_var)

n_features = X.shape[1]
n_classes = len(data[class_var].unique())

# load existing training data
print("Loading training data...")
X_train, Y_train, X_test, Y_test, X_validation, Y_validation= load_training_data( dataset_path )

print("====================Features====================")
print(feature_names)
print("================================================")

# the best performing model was obtained with 5 hidden layers with 12 neurons each
model_name = "model_h5_N12"

# specify paths where the blackbox model was saved
path_serialisation_model = PATH + "training/" + DATASET_NAME.replace(".csv", "") + "/model/" 
path_serialisation_histr = PATH + "training/" + DATASET_NAME.replace(".csv", "") + "/history/" 

# load model and model performance history
print("Loading Blackbox model...")
model_history = load_model_history( model_name, path_serialisation_histr )
model = load_model( model_name, path_serialisation_model )

# check modelxw
model.summary()

############# Initialise on Google Colab 

from google.colab import drive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials 
drive.mount('/content/gdrive', force_remount=True)
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
gdrive = GoogleDrive(gauth)
import sys
sys.path.append('/content/gdrive/MyDrive/Counterfactual-prototype-main/LORE/LORE-master')
#gdrive.CreateFile({"id": "1wwSN3AIl_dmayKENu5jnc1BRaNPe8BZc"}).GetContentFile("learning.py")

import sys
print(sys.path)
from pathlib import Path

Path('/content/gdrive/MyDrive/Counterfactual-prototype-main/LORE/LORE-master/yadt/dTcmd').is_file()

# print Working Directory 
!pwd

# Commented out IPython magic to ensure Python compatibility.
# %cd LORE-master







# Commented out IPython magic to ensure Python compatibility.
# %cd LORE-master
#%cd Counterfactual-prototype-main
#%cd LORE
#%cd
#%cd LORE-master

!ls

!chmod +x yadt/dTcmd

!pip install deap

import lore

from prepare_dataset import *
from neighbor_generator import *

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

warnings.filterwarnings("ignore")

def prepare_diabetes_dataset(file_path: str):
    
    '''
    [file_path]: path pointing the csv file.
    '''
    
    df = pd.read_csv(file_path)

    columns = df.columns.tolist()

    columns = columns[-1:] + columns[:-1]

    df = df[columns]

    df.head(5)

    class_name = 'Outcome'

    possible_outcomes = list(df[class_name].unique())

    type_features, features_type = recognize_features_type(df, class_name)

    discrete, continuous = set_discrete_continuous(columns, type_features, class_name, discrete=None, continuous=None)


    columns_tmp = list(columns)
    columns_tmp.remove(class_name)
    idx_features = {i: col for i, col in enumerate(columns_tmp)}

    df_le, label_encoder = label_encode(df, discrete)
    X = df_le.loc[:, df_le.columns != class_name].values
    y = df_le[class_name].values

    dataset = {
            'name': 'diabetes',
            'df': df,
            'columns': list(columns),
            'class_name': class_name,
            'possible_outcomes': possible_outcomes,
            'type_features': type_features,
            'features_type': features_type,
            'discrete': discrete,
            'continuous': continuous,
            'idx_features': idx_features,
            'label_encoder': label_encoder,
            'X': X,
            'y': y,
    }
    
    return dataset

path_data = '/content/gdrive/MyDrive/Counterfactual-prototype-main/datasets/'
# dataset = prepare_german_dataset(dataset_name, path_data)
dataset_name = 'diabetes.csv'
dataset = prepare_diabetes_dataset(path_data + dataset_name)

print(dataset['label_encoder'][dataset['class_name']].classes_)
print(dataset['possible_outcomes'])
X, y = dataset['X'], dataset['y']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# solve > binary research only need a output 
#seq

#class KeepOneValue(tf.keras.layers.Layer):
#    def __init__(self,):
#        super(KeepOneValue, self).__init__()

#    def call(self, inputs):
#        return inputs[:, 1:2]
#seq = tf.keras.Sequential(
#    [
#        model,
#        KeepOneValue(),
#    ]
#)



#Using this class to build a model and pass it 
class LOREWarpper(tf.keras.Model):
    def __init__(self, model):
        self.model = model
    def call(self, input):
        out = model(tf.constant(input))
        return out[:, 1:2]

seq = tf.keras.Model(
    [
        model,
        LOREWarpper(),
    ]
)
#super(ParallelModel, self).__init__()
#lOREWarpper = LOREWarpper(tf.keras.Model)

X2E = X_test
y2E = lOREWarpper.predict(X2E).flatten().astype(int)
#y2E = lOREWarpper.predict(X2E).flatten().astype(int)
#y2E= y2E[:,1].astype(int)


y2E = np.asarray([dataset['possible_outcomes'][i] for i in y2E])

idx_record2explain = 0

explanation, infos = lore.explain(idx_record2explain, X2E, dataset, lOREWarpper,
                                  ng_function=genetic_neighborhood,
                                  discrete_use_probabilities=True,
                                  continuous_function_estimation=False,
                                  returns_infos=True,
                                  path=path_data, sep=';', log=False)

dfX2E = build_df2explain(blackbox, X2E, dataset).to_dict('records')
dfx = dfX2E[idx_record2explain]
# x = build_df2explain(blackbox, X2E[idx_record2explain].reshape(1, -1), dataset).to_dict('records')[0]

print('x = %s' % dfx)
print('r = %s --> %s' % (explanation[0][1], explanation[0][0]))
for delta in explanation[1]:
    print('delta', delta)

covered = lore.get_covered(explanation[0][1], dfX2E, dataset)
print(len(covered))
print(covered)

print(explanation[0][0][dataset['class_name']], '<<<<')

def eval(x, y):
    return 1 if x == y else 0

#precision = [1-eval(v, explanation[0][0][dataset['class_name']]) for v in y2E[covered]]
#print(precision)
#print(np.mean(precision), np.std(precision))



