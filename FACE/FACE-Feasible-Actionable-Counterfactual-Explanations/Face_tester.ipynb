{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Face_tester.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM1up4xmlYLfCKkABl77XuF"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"BE9N7UoFhVAI"},"source":["# FACE "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Q0MnDLUhQ5Y","executionInfo":{"status":"ok","timestamp":1623312200025,"user_tz":-600,"elapsed":2310,"user":{"displayName":"Leon C","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GirxcSFELKJsfIAZZuEJ9ZecG__1hzk1e5zOJAqUA=s64","userId":"16235014008629953036"}},"outputId":"708c0c24-0889-4c1b-866b-e8216eb41caf"},"source":["############# Initialise on Google Colab \n","from google.colab import drive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials \n","drive.mount('/content/gdrive', force_remount=True)\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","gdrive = GoogleDrive(gauth)\n","\n","createdFile = gdrive.CreateFile({\"id\": \"1wwSN3AIl_dmayKENu5jnc1BRaNPe8BZc\"})\n","createdFile.GetContentFile(\"utils.py\")\n","createdFile.GetContentFile(\"distribution.py\")\n","createdFile.GetContentFile(\"distribution.py\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wC0imFFsp8g9"},"source":["#  !pip3 install pyAgrum\n","# !pip install alibi\n","## Removing str encoding error.\n","# !python3 -m pip install 'h5py==2.10.0' --force-reinstall"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mn51Q5dLhhNF","executionInfo":{"status":"ok","timestamp":1623312225367,"user_tz":-600,"elapsed":307,"user":{"displayName":"Leon C","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GirxcSFELKJsfIAZZuEJ9ZecG__1hzk1e5zOJAqUA=s64","userId":"16235014008629953036"}},"outputId":"e573baf6-0cca-4829-ee9e-fd4dc3cdda42"},"source":["import tensorflow as tf\n","# tf.get_logger().setLevel(40) # suppress deprecation messages\n","# tf.compat.v1.disable_v2_behavior() # disable TF2 behaviour as alibi code still relies on TF1 constructs\n","from tensorflow.keras.layers import Dense, Input\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.utils import to_categorical\n","import matplotlib\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","from sklearn.datasets import load_boston\n","#from alibi.explainers import CounterFactualProto, CounterFactual\n","import pandas as pd\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler,MinMaxScaler\n","\n","\n","#from learning import *\n","from time import time\n","#from learning import encode_data \n","\n","print('TF version: ', tf.__version__)\n","print('Eager execution enabled: ', tf.executing_eagerly()) # False\n","seed = 123\n","tf.random.set_seed(seed)\n","np.random.seed(seed)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TF version:  2.5.0\n","Eager execution enabled:  True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OfyjALglhtkS"},"source":["# Load dataset"]},{"cell_type":"code","metadata":{"id":"eAXHhUWfhhPk"},"source":["from google.colab import drive \n","# drive.mount('/content/gdrive') \n","data=pd.read_csv('gdrive/My Drive//Counterfactual-prototype-main/datasets/diabetes.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":223},"id":"PYb9pSW3hhSI","executionInfo":{"status":"error","timestamp":1623312241439,"user_tz":-600,"elapsed":307,"user":{"displayName":"Leon C","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GirxcSFELKJsfIAZZuEJ9ZecG__1hzk1e5zOJAqUA=s64","userId":"16235014008629953036"}},"outputId":"10cd6400-ce07-4c51-dc98-0a001bfbf2c3"},"source":["# Giving current root path\n","# PATH = \"./\"\n","PATH = \"gdrive/My Drive//Counterfactual-prototype-main/\"\n","# PATH = \"/Counterfactual-prototype-main/\"\n","# name of dataset\n","DATASET_NAME = \"diabetes.csv\"\n","\n","# variable containing the class labels in this case the dataset contains:\n","# 0 - if not diabetes\n","# 1 - if diabetes\n","class_var = \"Outcome\"\n","\n","# load dataset\n","dataset_path = PATH + \"datasets/\" + DATASET_NAME\n","data = pd.read_csv( dataset_path )\n","\n","# features\n","feature_names = data.drop([class_var], axis=1).columns.to_list()\n","\n","# balance dataset\n","sampled_data = data.sample(frac=1)\n","sampled_data = sampled_data[ sampled_data[\"Outcome\"] == 0]\n","\n","no_data = sampled_data.sample(frac=1)[0:268]\n","yes_data = data[ data[\"Outcome\"] == 1]\n","\n","balanced_data = [no_data,yes_data]\n","balanced_data = pd.concat(balanced_data)\n","\n","# apply one hot encoder to data\n","# standardize the input between 0 and 1\n","X, Y, encoder, scaler = encode_data( balanced_data, class_var)\n","\n","n_features = X.shape[1]\n","n_classes = len(data[class_var].unique())\n","\n","# load existing training data\n","print(\"Loading training data...\")\n","X_train, Y_train, X_test, Y_test, X_validation, Y_validation= load_training_data( dataset_path )\n","\n","print(\"====================Features====================\")\n","print(feature_names)\n","print(\"================================================\")"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-bb117bd76b13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# apply one hot encoder to data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# standardize the input between 0 and 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_data\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mbalanced_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'encode_data' is not defined"]}]},{"cell_type":"code","metadata":{"id":"nG3zwbcvhz2D"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pZy1gQ87hz4I","executionInfo":{"status":"ok","timestamp":1623313972733,"user_tz":-600,"elapsed":1330,"user":{"displayName":"Leon C","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GirxcSFELKJsfIAZZuEJ9ZecG__1hzk1e5zOJAqUA=s64","userId":"16235014008629953036"}},"outputId":"14e2ed4a-f58b-413b-9dd0-116544d0e02e"},"source":["############# Initialise on Google Colab \n","from google.colab import drive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials \n","drive.mount('/content/gdrive', force_remount=True)\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","gdrive = GoogleDrive(gauth)\n","\n","# createdFile = gdrive.CreateFile({\"id\": \"1wwSN3AIl_dmayKENu5jnc1BRaNPe8BZc\"})\n","# createdFile.GetContentFile(\"utils.py\")\n","# createdFile.GetContentFile(\"distribution.py\")\n","# createdFile.GetContentFile(\"FACE.py\")\n","# createdFile.GetContentFile(\"Feasibility.py\")\n","# createdFile.GetContentFile(\"face_dataLoader.py\")\n","# createdFile.GetContentFile(\"kernel.py\")\n","\n","import sys\n","sys.path.append('/content/gdrive/My Drive//Counterfactual-prototype-main/FACE/FACE-Feasible-Actionable-Counterfactual-Explanations/')\n","sys.path.append('/content/gdrive/My Drive//Counterfactual-prototype-main/')\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":426},"id":"CD963buHe4MK","executionInfo":{"status":"error","timestamp":1623313982363,"user_tz":-600,"elapsed":2064,"user":{"displayName":"Leon C","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GirxcSFELKJsfIAZZuEJ9ZecG__1hzk1e5zOJAqUA=s64","userId":"16235014008629953036"}},"outputId":"09a452e3-fa68-4f2d-c3c4-2d42a384f1e8"},"source":["#!/bin/sh\n","\n","import sys\n","import numpy as np\n","import pandas as pd\n","import pickle as pk\n","import random\n","\n","from matplotlib import pyplot as plt\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KernelDensity\n","from scipy.special import gamma as gamma_function\n","from sklearn import preprocessing\n","\n","import utils\n","import distribution\n","from FACE import *\n","from kernel import *\n","from face_dataLoader import *\n","\n","EPSILON = 1e-8\n","\n","class distance_obj:\n","\tdef __init__(self):\n","\t\treturn\n","\n","\tdef computeDistance(self, xi, xj):\n","\t\tdist = np.linalg.norm(xi - xj, 2)\n","\t\treturn dist\n","\n","def plot_recourse(data, face_recourse, plot_idx=0):\n","\tall_pos = data[data.y == 1]\n","\tall_pos_x1 = all_pos.x1.values\n","\tall_pos_x2 = all_pos.x2.values\n","\n","\tall_neg = data[data.y == 0]\n","\tall_neg_x1 = all_neg.x1.values\n","\tall_neg_x2 = all_neg.x2.values\n","\n","\tplt.plot(all_pos_x1, all_pos_x2, '*')\n","\tplt.plot(all_neg_x1, all_neg_x2, '*')\n","\tplt.xlabel('x1')\n","\tplt.ylabel('x2')\n","\tplt.title('FACE Synthetic data set')\n","\n","\tassert(plot_idx < len(data))\n","\n","\tplot_pt = face_recourse[plot_idx]['factual_instance']\n","\tplot_cfpt = face_recourse[plot_idx]['counterfactual_target']\n","\tpoints_x1 = []\n","\tpoints_x2 = []\n","\tpoints_x1 = [data.iloc[x]['x1'] for x in face_recourse[plot_idx]['path']]\n","\tpoints_x2 = [data.iloc[x]['x2'] for x in face_recourse[plot_idx]['path']]\n","\tplt.plot(points_x1, points_x2, color='green')\n","\tplt.plot(plot_pt['x1'], plot_pt['x2'], 'o',color='red')\n","\tplt.plot(plot_cfpt['x1'], plot_cfpt['x2'], 'o', color='red')\n","\tplt.savefig('./tmp/recourse_path_{}.jpg'.format(plot_idx))\n","\n","def data_statistics(data, distance, datasetName):\n","\tprint(\"Computing Data Statistics ...\")\n","\n","\tdists = []\n","\tN = len(data)\n","\tfor i in range(0, N):\n","\t\tfor j in range(i+1, N):\n","\t\t\txi = data.iloc[i]\n","\t\t\txj = data.iloc[j]\n","\t\t\tdist = distance.computeDistance(xi, xj)\n","\t\t\tdists.append(dist)\n","\n","\tplt.hist(dists, bins='auto')\n","\tplt.title(\"Distribution of distances in the graph for given dataset\")\n","\tplt.xlabel(\"distance\")\n","\tplt.ylabel(\"number of points\")\n","\tplt.savefig(\"data_statistics_{}.png\".format(datasetName))\n","\t\n","\t## Calculate important statistics\n","\tmean = np.mean(dists)\n","\tvar = np.var(dists)\n","\tp10 = np.percentile(dists, 10)\n","\tp25 = np.percentile(dists, 25)\n","\tp50 = np.percentile(dists, 50)\n","\tp75 = np.percentile(dists, 75)\n","\tmin_ = np.min(dists)\n","\tmax_ = np.max(dists)\n","\n","\tprint(\"Statistics:\\n mean: {}\\n var: {}\\n p10: {}\\n p25: {}\\n p50: {}\\n p75: {}\\n min: {}\\n max: {}\".format(mean, var, p10, p25, p50, p75, min_, max_))\n","\n","def main_synthetic_face(epsilon=0.2, tp=0.6, td=0.001):\n","\t\"\"\"\n","\ttp = 0.6 # Prediction threshold\n","\ttd = 0.001 # density threshold\n","\tepsilon = 0.18 # margin for creating connections in graphs\n","\t\"\"\"\n","\t# dataPath = \"./data/synthetic_one_hot\"\n","\t# datasetName = 'synthetic_lin'\n","\t# FEATURE_COLUMNS = ['x1', 'x2', 'x3']\n","\t\n","\tdatasetName = 'synthetic_one_hot'\n","\tdata, FEATURE_COLUMNS, TARGET_COLUMNS = load_synthetic_one_hot()\n","\tX = data[FEATURE_COLUMNS]\n","\ty = data[TARGET_COLUMNS]\n","\n","\t### Train a logistic regression model\n","\tclf = LogisticRegression(random_state=utils.random_seed)\n","\tclf.fit(X, y)\n","\tprint(\"Training accuracy:\", clf.score(X, y))\n","\n","\t### Get the negatively classified points\n","\tnegative_points = utils.get_negatively_classified(data, clf, FEATURE_COLUMNS)\n","\tprint(\"# negative points:\", len(negative_points))\n","\n","\t### Initialize FACE object\n","\tdistrib = distribution.distribution(data)\n","\tkernel = Kernel_obj(distrib, Num_points=len(data), knnK=5)\n","\tkernel.fitKernel(X)\n","\n","\tdist_obj = distance_obj()\n","\tface = FACE(data, distrib, dist_obj, kernel, FEATURE_COLUMNS, TARGET_COLUMNS, epsilon, clf)\n","\tfeasibility_constraints = utils.getFeasibilityConstraints(FEATURE_COLUMNS, dataset_name=datasetName)\n","\tface.make_graph(feasibility_constraints, epsilon)\n","\n","\trecourse_points = {}\n","\tpath_lengths = []\n","\tfor n_id, n in enumerate(negative_points):\n","\t\t# if (n_id > 250):\n","\t\t# \tbreak\n","\t\tprint(\"Computing recourse for: {}/{}\".format(n_id, len(negative_points)))\n","\t\trecourse_point, cost, recourse_path = face.compute_recourse(n, tp, td)\n","\n","\t\trecourse_points[n_id] = {}\n","\t\trecourse_points[n_id]['name'] = n\n","\t\trecourse_points[n_id]['factual_instance'] = negative_points[n]\n","\t\trecourse_points[n_id]['counterfactual_target'] = recourse_point\n","\t\trecourse_points[n_id]['cost'] = cost\n","\t\trecourse_points[n_id]['path'] = recourse_path\n","\t\tif (recourse_path is not None):\n","\t\t\tpath_lengths.append(len(recourse_path))\n","\n","\t# print(recourse_points)\n","\tpk.dump(clf, open(\"./tmp/LR_classifier_face_{}_KDEkernel_eps{}_td{}.pk\".format(datasetName, epsilon, td), 'wb'))\n","\tpk.dump(recourse_points, open(\"./tmp/Face_recourse_points_KDEkernel_{}_eps{}_td{}.pk\".format(datasetName, epsilon, td), 'wb'))\n","\tprint(\"Mean Path length:\", np.mean(path_lengths))\n","\tprint(\"Median Path Length:\", np.median(path_lengths))\n","\n","\t### Plot recourse for 10th data point\n","\t# plot_recourse(data, recourse_points, 10)\n","\treturn recourse_points, np.median(path_lengths)\n","\n","def cross_validate_path_length(datasetName='synthetic_face'):\n","\tloop_name = 'tp'\n","\tpath_lengths = {}\n","\tpath_scores = {}\n","\tif (loop_name == 'td'):\n","\t\ttd_range = np.arange(0.15, 0.42, 0.05)\n","\t\tfor _td in td_range:\n","\t\t\tprint(\"td:\", _td)\n","\t\t\tr_points, m_path_len = main(epsilon=0.2, td=_td, datasetName=datasetName)\n","\t\t\tpath_lengths[_td] = np.mean(m_path_len)\n","\t\t\tpath_scores[_td] = np.mean(m_path_len)*len(m_path_len)/len(r_points)\n","\tif (loop_name == 'tp'):\n","\t\ttp_range = np.arange(0.55, 0.76, 0.05)\n","\t\tfor _tp in tp_range:\n","\t\t\tprint(\"tp:\", _tp)\n","\t\t\tr_points, m_path_len = main(epsilon=0.2, td=_tp, datasetName=datasetName)\n","\t\t\tpath_lengths[_tp] = np.mean(m_path_len)\n","\t\t\tpath_scores[_tp] = np.mean(m_path_len)*len(m_path_len)/len(r_points)\n","\tif (loop_name == 'eps'):\n","\t\t# eps_range = [0.18, 0.22, 0.26, 0.30, 0.34, 0.38]\n","\t\teps_range = [0.4, 0.45, 0.5]\n","\t\tfor eps in eps_range:\n","\t\t\tprint(\"eps:\", eps)\n","\t\t\tr_points, m_path_len = main(epsilon=eps, datasetName=datasetName)\n","\t\t\tpath_lengths[eps] = np.mean(m_path_len)\n","\t\t\tpath_scores[eps] = np.mean(m_path_len)*len(m_path_len)/len(r_points)\n","\n","\t\n","\tpk.dump(path_scores, open(\"tmp/path_scores_variation_{}_{}.pk\".format(loop_name, datasetName), \"wb\"))\n","\tprint(\"Path lengths:\", path_lengths)\n","\tprint(\"PathLength*fracRecourseProvided:\", path_scores)\n","\tlists = path_scores.items()\n","\tx, y = zip(*lists)\n","\tplt.plot(x, y)\n","\tplt.xlabel(loop_name)\n","\tplt.ylabel(\"Recourse Path Scores\")\n","\tplt.title(\"PathLength*fracRecourseProvided variation with {}\".format(loop_name))\n","\tplt.savefig(\"Path_Score_variation_{}_{}.jpg\".format(loop_name, datasetName))\n","\n","def main(epsilon=0.2, tp=0.6, td=0.001, datasetName='german_credit', expIter='0.0'):\n","\t\"\"\"\n","\ttp = 0.6 # Prediction threshold\n","\ttd = 0.0001 # density threshold\n","\tepsilon = 0.3 # margin for creating connections in graphs\n","\t# dataPath = \"./data/synthetic_one_hot\"\n","\t# datasetName = 'synthetic_lin'\n","\t# FEATURE_COLUMNS = ['x1', 'x2', 'x3']\n","\t\"\"\"\n","\n","\tdata, FEATURE_COLUMNS, TARGET_COLUMNS = load_dataset(datasetName=datasetName)\n","\tTEST_SIZE = 0.3\n","\n","\tX = data[FEATURE_COLUMNS]\n","\ty = data[TARGET_COLUMNS]\n","\n","\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=utils.random_seed, shuffle=True)\n","\tdata_train = pd.concat([y_train, X_train], axis=1)\n","\tdata_test = pd.concat([y_test, X_test], axis=1)\n","\tprint(\"Data train:\", data_train.shape)\n","\tprint(\"Data Train columns:\", data_train.columns)\n","\n","\t### Train a logistic regression model\n","\tclf = LogisticRegression(random_state=utils.random_seed)\n","\tclf.fit(X_train, y_train)\n","\tprint(\"Training accuracy:\", clf.score(X_train, y_train))\n","\tprint(\"Testing accuracy:\", clf.score(X_test, y_test))\n","\n","\t### Get the negatively classified points\n","\tnegative_points = utils.get_negatively_classified(data_test, clf, FEATURE_COLUMNS)\n","\tprint(\"# negative points:\", len(negative_points))\n","\n","\t### Initialize FACE object\n","\tdist_obj = distance_obj()\n","\t# data_statistics(X, dist_obj, datasetName=datasetName) ## Get data statistics\n","\t# distrib = distribution.distribution(data)\n","\tdistrib = distribution.kernel_distribution(data)\n","\t# distrib = distribution.synthetic_distribution_face(data)\n","\t# print(\"PDF:\", distrib.pdf(data.iloc[0][FEATURE_COLUMNS], data.iloc[0][TARGET_COLUMNS]))\n","\tkernel = Kernel_obj(distrib, Num_points=len(data))\t\n","\tkernel.fitKernel(X)\n","\tdistrib.setKernel(kernel)\n","\n","\tface = FACE(data, distrib, dist_obj, kernel, FEATURE_COLUMNS, TARGET_COLUMNS, epsilon, clf)\n","\tfeasibility_constraints = utils.getFeasibilityConstraints(FEATURE_COLUMNS, dataset_name=datasetName)\n","\tface.make_graph(feasibility_constraints, epsilon)\n","\n","\trecourse_points = {}\n","\tpath_lengths = []\n","\tfor n_id, n in enumerate(negative_points):\n","\t\tprint(\"Computing recourse for: {}/{}\".format(n_id, len(negative_points)))\n","\t\trecourse_point, cost, recourse_path = face.compute_recourse(n, tp, td)\n","\n","\t\trecourse_points[n_id] = {}\n","\t\trecourse_points[n_id]['name'] = n\n","\t\trecourse_points[n_id]['factual_instance'] = negative_points[n]\n","\t\trecourse_points[n_id]['counterfactual_target'] = recourse_point\n","\t\trecourse_points[n_id]['cost'] = cost\n","\t\trecourse_points[n_id]['path'] = recourse_path\n","\t\tif (recourse_path is not None):\n","\t\t\tpath_lengths.append(len(recourse_path))\n","\n","\t# print(recourse_points)\n","\tpk.dump(clf, open(\"./tmp/LR_classifier_face_data{}_eps{}_tp{}_td{}_expIter{}.pk\".format(datasetName, epsilon, tp, td, expIter), 'wb'))\n","\tpk.dump(recourse_points, open(\"./tmp/Face_recourse_points_{}_eps{}_tp{}_td{}_expIter{}.pk\".format(datasetName, epsilon, tp, td, expIter), 'wb'))\n","\n","\tprint(\"Mean Path length:\", np.mean(path_lengths))\n","\tprint(\"Recourse Found:\", len(path_lengths)/len(negative_points))\n","\tprint(\"Median Path Length:\", np.median(path_lengths))\n","\n","\treturn recourse_points, path_lengths\n","\n","def main_train_test(epsilon=0.2, tp=0.6, td=0.001, datasetName='german_credit', expIter='0.0'):\n","\t\"\"\"\n","\ttp = 0.6 # Prediction threshold\n","\ttd = 0.0001 # density threshold\n","\tepsilon = 0.3 # margin for creating connections in graphs\n","\t# dataPath = \"./data/synthetic_one_hot\"\n","\t# datasetName = 'synthetic_lin'\n","\t# FEATURE_COLUMNS = ['x1', 'x2', 'x3']\n","\t\"\"\"\n","\n","\tdata, data_recourse_test, FEATURE_COLUMNS, TARGET_COLUMNS = load_german_synthetic_sampled_dataset()\n","\tTEST_SIZE = 0.8\n","\n","\tX = data[FEATURE_COLUMNS]\n","\ty = data[TARGET_COLUMNS]\n","\n","\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=utils.random_seed, shuffle=True)\n","\tdata_train = pd.concat([y_train, X_train], axis=1)\n","\tdata_test = pd.concat([y_test, X_test], axis=1)\n","\tprint(\"Data train:\", data_train.shape)\n","\tprint(\"Data Train columns:\", data_train.columns)\n","\n","\t### Train a logistic regression model\n","\tclf = LogisticRegression(random_state=utils.random_seed)\n","\tclf.fit(X_train, y_train)\n","\tprint(\"Training accuracy:\", clf.score(X_train, y_train))\n","\tprint(\"Testing accuracy:\", clf.score(X_test, y_test))\n","\n","\t### Get the negatively classified points\n","\tnegative_points = utils.get_negatively_classified(data_recourse_test, clf, FEATURE_COLUMNS)\n","\tprint(\"# negative points:\", len(negative_points) , \"/\", len(data_recourse_test))\n","\n","\t### Initialize FACE object\n","\tdist_obj = distance_obj()\n","\t# data_statistics(X, dist_obj, datasetName=datasetName) ## Get data statistics\n","\t# distrib = distribution.distribution(data)\n","\tdistrib = distribution.kernel_distribution(data)\n","\t# distrib = distribution.synthetic_distribution_face(data)\n","\t# print(\"PDF:\", distrib.pdf(data.iloc[0][FEATURE_COLUMNS], data.iloc[0][TARGET_COLUMNS]))\n","\tkernel = Kernel_obj(distrib, Num_points=len(data))\n","\tkernel.fitKernel(X)\n","\tdistrib.setKernel(kernel)\n","\n","\tface = FACE(data, distrib, dist_obj, kernel, FEATURE_COLUMNS, TARGET_COLUMNS, epsilon, clf)\n","\tfeasibility_constraints = utils.getFeasibilityConstraints(FEATURE_COLUMNS, dataset_name=datasetName)\n","\tface.make_graph(feasibility_constraints, epsilon)\n","\n","\trecourse_points = {}\n","\tpath_lengths = []\n","\tfor n_id, n in enumerate(negative_points):\n","\t\tprint(\"Computing recourse for: {}/{}\".format(n_id, len(negative_points)))\n","\t\trecourse_point, cost, recourse_path = face.compute_recourse(n, tp, td)\n","\n","\t\trecourse_points[n_id] = {}\n","\t\trecourse_points[n_id]['name'] = n\n","\t\trecourse_points[n_id]['factual_instance'] = negative_points[n]\n","\t\trecourse_points[n_id]['counterfactual_target'] = recourse_point\n","\t\trecourse_points[n_id]['cost'] = cost\n","\t\trecourse_points[n_id]['path'] = recourse_path\n","\t\tif (recourse_path is not None):\n","\t\t\tpath_lengths.append(len(recourse_path))\n","\n","\t# print(recourse_points)\n","\tpk.dump(clf, open(\"./tmp/LR_classifier_face_data{}_eps{}_tp{}_td{}_expIter{}.pk\".format(datasetName, epsilon, tp, td, expIter), 'wb'))\n","\tpk.dump(recourse_points, open(\"./tmp/Face_recourse_points_{}_eps{}_tp{}_td{}_expIter{}.pk\".format(datasetName, epsilon, tp, td, expIter), 'wb'))\n","\n","\tprint(\"Mean Path length:\", np.mean(path_lengths))\n","\tprint(\"Recourse Found:\", len(path_lengths)/len(negative_points))\n","\tprint(\"Median Path Length:\", np.median(path_lengths))\n","\n","\treturn recourse_points, path_lengths\n","\n","def unit_test():\n","\tdataPath = \"./data/synthetic_face_dataset.pk\"\n","\tNum_Features = 2\n","\tFEATURE_COLUMNS = [('x' + str(i+1)) for i in range(0, len(Num_Features))]\n","\tTARGET_COLUMNS = ['y']\n","\tepsilon = 0.05 # margin for creating connections in graphs\n","\tdata = pk.load(open(dataPath, 'rb'))\n","\tclf = LogisticRegression(random_state=utils.random_seed)\n","\tdistrib = distribution.distribution(data)\n","\tkernel = Kernel_obj(distrib)\n","\tdist_obj = distance_obj()\n","\tface = FACE(data, distrib, dist_obj, kernel, FEATURE_COLUMNS, TARGET_COLUMNS, epsilon, clf)\n","\tface.unit_test_djikstra()\n","\n","if __name__==\"__main__\":\n","\t# main_train_test(epsilon=0.35, td=0.001, tp=0.55, datasetName='synthetic_german_one_hot_sampled', expIter='4.1.3')\n","\tmain_synthetic_face(epsilon=0.2)\n","\t# cross_validate_path_length(datasetName='synthetic_german_one_hot')\n","\t# cross_validate_path_length(datasetName='synthetic_face')\n","\t# unit_test()\n"],"execution_count":8,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-9b27b3c8b462>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;31m# main_train_test(epsilon=0.35, td=0.001, tp=0.55, datasetName='synthetic_german_one_hot_sampled', expIter='4.1.3')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mmain_synthetic_face\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;31m# cross_validate_path_length(datasetName='synthetic_german_one_hot')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;31m# cross_validate_path_length(datasetName='synthetic_face')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-9b27b3c8b462>\u001b[0m in \u001b[0;36mmain_synthetic_face\u001b[0;34m(epsilon, tp, td)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mdatasetName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'synthetic_one_hot'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFEATURE_COLUMNS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTARGET_COLUMNS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_synthetic_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFEATURE_COLUMNS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTARGET_COLUMNS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/gdrive/My Drive//Counterfactual-prototype-main/FACE/FACE-Feasible-Actionable-Counterfactual-Explanations/face_dataLoader.py\u001b[0m in \u001b[0;36mload_synthetic_one_hot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \t'''\n\u001b[1;32m     15\u001b[0m         \u001b[0mdataPath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../robustnessExperiments/data/synthetic_exp4/synthetic_one_hot\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_frame_kurz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0msample_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mdata_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../robustnessExperiments/data/synthetic_exp4/synthetic_one_hot'"]}]},{"cell_type":"code","metadata":{"id":"hfe_zK3483iY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RuY2AThkgPMt"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YVYkP4h2hP6w"},"source":[""],"execution_count":null,"outputs":[]}]}